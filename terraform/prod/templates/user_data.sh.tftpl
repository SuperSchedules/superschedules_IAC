#!/usr/bin/env bash
set -euxo pipefail

REGION=${region}
APP_DIR=/opt/superschedules
mkdir -p "$APP_DIR"

# Create swap file (2GB for t3.micro with 1GB RAM)
if [ ! -f /swapfile ]; then
  dd if=/dev/zero of=/swapfile bs=1M count=2048 status=progress
  chmod 600 /swapfile
  mkswap /swapfile
  swapon /swapfile
  echo '/swapfile none swap sw 0 0' >> /etc/fstab
  # Set swappiness to 10 (prefer RAM, use swap as backup)
  sysctl vm.swappiness=10
  echo 'vm.swappiness=10' >> /etc/sysctl.conf
fi

# Install Docker and compose plugin
if command -v apt-get >/dev/null 2>&1; then
  export DEBIAN_FRONTEND=noninteractive
  apt-get update -y
  apt-get install -y ca-certificates curl gnupg lsb-release awscli jq
  install -m 0755 -d /etc/apt/keyrings
  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg
  chmod a+r /etc/apt/keyrings/docker.gpg
  echo \
    "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable" | \
    tee /etc/apt/sources.list.d/docker.list >/dev/null
  apt-get update -y
  apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
  systemctl enable docker
  systemctl start docker
fi

# ECR login
aws ecr get-login-password --region "$REGION" | docker login --username AWS --password-stdin ${aws_account_id}.dkr.ecr.${region}.amazonaws.com

# Fetch secrets from AWS Secrets Manager
SECRET_JSON=$(aws secretsmanager get-secret-value \
  --secret-id prod/superschedules/secrets \
  --region "$REGION" \
  --query SecretString \
  --output text)

DB_PASSWORD=$(echo "$SECRET_JSON" | jq -r '.DB_PASSWORD')
DJANGO_SECRET_KEY=$(echo "$SECRET_JSON" | jq -r '.DJANGO_SECRET_KEY')
EMAIL_HOST_USER=$(echo "$SECRET_JSON" | jq -r '.EMAIL_HOST_USER')
EMAIL_HOST_PASSWORD=$(echo "$SECRET_JSON" | jq -r '.EMAIL_HOST_PASSWORD')
TURNSTILE_SECRET_KEY=$(echo "$SECRET_JSON" | jq -r '.TURNSTILE_SECRET_KEY // empty')

cat > "$APP_DIR/docker-compose.yml" <<YAML
services:
  frontend:
    image: ${nginx_image}
    ports:
      - "80:80"
    logging:
      driver: awslogs
      options:
        awslogs-region: ${region}
        awslogs-group: /aws/superschedules/prod/app
        awslogs-stream: frontend
        awslogs-create-group: "true"
  django:
    image: ${django_image}
    ports:
      - "8000:8000"
    environment:
      - DJANGO_SETTINGS_MODULE=${django_settings_module}
      - DJANGO_SECRET_KEY=$DJANGO_SECRET_KEY
      - DB_HOST=${db_host}
      - DB_PORT=${db_port}
      - DB_NAME=${db_name}
      - DB_USER=${db_username}
      - DB_PASSWORD=$DB_PASSWORD
      - AWS_REGION=${region}
      - USE_SQS_BROKER=True
      - AWS_S3_BUCKET=${static_bucket}
      - ALB_HOST=${alb_dns_name}
      - DEBUG=False
      - LLM_PROVIDER=bedrock
      - AWS_BEDROCK_REGION=${region}
      - EMAIL_HOST_USER=$EMAIL_HOST_USER
      - EMAIL_HOST_PASSWORD=$EMAIL_HOST_PASSWORD
      - DEFAULT_FROM_EMAIL=noreply@eventzombie.com
      - EMAIL_HOST=email-smtp.us-east-1.amazonaws.com
      - EMAIL_PORT=587
      - EMAIL_USE_TLS=True
      - FRONTEND_URL=https://eventzombie.com
      - TURNSTILE_SECRET_KEY=$TURNSTILE_SECRET_KEY
    depends_on:
      - wait-for-db
    logging:
      driver: awslogs
      options:
        awslogs-region: ${region}
        awslogs-group: /aws/superschedules/prod/app
        awslogs-stream: django
        awslogs-create-group: "true"
  celery-worker:
    image: ${django_image}
    command: celery -A config worker --loglevel=INFO --concurrency=1 --queues=default,embeddings,geocoding,scraping
    environment:
      - DJANGO_SETTINGS_MODULE=${django_settings_module}
      - DJANGO_SECRET_KEY=$DJANGO_SECRET_KEY
      - DB_HOST=${db_host}
      - DB_PORT=${db_port}
      - DB_NAME=${db_name}
      - DB_USER=${db_username}
      - DB_PASSWORD=$DB_PASSWORD
      - AWS_REGION=${region}
      - USE_SQS_BROKER=True
      - AWS_S3_BUCKET=${static_bucket}
      - EMAIL_HOST_USER=$EMAIL_HOST_USER
      - EMAIL_HOST_PASSWORD=$EMAIL_HOST_PASSWORD
      - DEFAULT_FROM_EMAIL=noreply@eventzombie.com
      - EMAIL_HOST=email-smtp.us-east-1.amazonaws.com
      - EMAIL_PORT=587
      - EMAIL_USE_TLS=True
    depends_on:
      - wait-for-db
    restart: unless-stopped
    logging:
      driver: awslogs
      options:
        awslogs-region: ${region}
        awslogs-group: /aws/superschedules/prod/app
        awslogs-stream: celery-worker
        awslogs-create-group: "true"
  wait-for-db:
    image: public.ecr.aws/bitnami/postgresql:15
    command: ["/bin/bash", "-lc", "until pg_isready -h ${db_host} -p ${db_port} -U ${db_username}; do echo waiting for db; sleep 3; done"]
YAML

cat > /etc/systemd/system/superschedules.service <<UNIT
[Unit]
Description=Superschedules stack
After=docker.service
Requires=docker.service

[Service]
Type=oneshot
RemainAfterExit=yes
WorkingDirectory=/opt/superschedules
ExecStartPre=/bin/bash -c 'aws ecr get-login-password --region ${region} | docker login --username AWS --password-stdin ${aws_account_id}.dkr.ecr.${region}.amazonaws.com'
ExecStart=/usr/bin/docker compose -f /opt/superschedules/docker-compose.yml up -d --pull always
ExecStop=/usr/bin/docker compose -f /opt/superschedules/docker-compose.yml down
TimeoutStartSec=0
Restart=on-failure

[Install]
WantedBy=multi-user.target
UNIT

systemctl daemon-reload
systemctl enable superschedules
systemctl start superschedules

# Wait for services to be healthy
sleep 30

# Enable pgvector extension (idempotent)
DJANGO_CONTAINER=$(docker compose -f /opt/superschedules/docker-compose.yml ps -q django)
docker exec "$DJANGO_CONTAINER" python manage.py shell -c "from django.db import connection; cursor = connection.cursor(); cursor.execute('CREATE EXTENSION IF NOT EXISTS vector;'); print('pgvector extension enabled')" || echo "pgvector extension creation failed"

# Run Django migrations (creates tables and enables pgvector)
docker exec "$DJANGO_CONTAINER" python manage.py migrate --noinput || echo "Migration failed - may need manual intervention"

echo "Bootstrapping complete"

